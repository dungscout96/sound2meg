{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sccn/sound2meg/blob/main/Spatial_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DvwsUf4jh2bC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from os import listdir\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "filePath = '/content/drive/MyDrive/sound2meg/'\n",
    "filePath = '/expanse/projects/nsg/external_users/public/arno/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sound2MEGDataset(Dataset):\n",
    "  def __init__(self, path):\n",
    "    #self.wav_files = wav_files\n",
    "    self.path = path\n",
    "    self.sizes = np.empty(0, dtype=int)\n",
    "    self.subjects = []\n",
    "    for subject in range(0, 126):\n",
    "        if os.path.exists(filePath + 'MEG_Signals/S%03dT000.npy'%subject):\n",
    "            files = glob.glob(filePath + 'MEG_Signals/S%03dT*.npy'%subject)\n",
    "            self.sizes = np.append(self.sizes, len(files))\n",
    "            self.subjects.append( '%03d'%subject )\n",
    "                                \n",
    "    self.filename = []\n",
    "#     for file in listdir(path + 'mat_files'):\n",
    "#       self.subjects.append(file[6:9])             #Making a list of all the mat files\n",
    "  def __len__(self):\n",
    "    return sum(self.sizes)\n",
    "  def __getitem__(self, idx):\n",
    "    for i in range(len(self.sizes)):              #Taking a cummulative sum of all the sizes\n",
    "      if np.cumsum(self.sizes)[i] > idx:          #This way we will know the ranges of the different MEG Signal samples in the context of the entire dataset\n",
    "        idx_file = i                              #For example, say the sizes of the mat files are [3, 5, 2]\n",
    "        break                                     #Using the cummulative function, we get [3, 8, 10]\n",
    "    #mat_file = loadmat(self.path + 'mat_files/' + self.filename[idx_file]) #This tells that samples 0 to 2 are in the first file, 3 to 7 in the second and so on...\n",
    "    if idx_file == 0:                             #Accordingly, we see which sample is in which range by finding the smallest value in the cumsum larger than idx                           \n",
    "      idx_sound = idx\n",
    "    else:\n",
    "      idx_sound = idx - np.cumsum(self.sizes)[idx_file - 1]\n",
    "    \n",
    "    fileName = self.path + 'MEG_Signals/S'+self.subjects[idx_file]+'T%03d.npy'%idx_sound\n",
    "    if os.path.isfile(fileName):\n",
    "        MEG_Signal = np.load(fileName)\n",
    "        with open(self.path + 'MEG_Signals/audios.csv', 'r') as f:\n",
    "          audios = list(csv.reader(f))\n",
    "          audio_file = audios[idx_file][idx_sound][2:5]\n",
    "        #Finding the associated audiofile, and extracting only the number of the file out of it\n",
    "        #audio_file = mat_file['audiofiles'][0, idx_sound][0][0:3]\n",
    "        #The filenames are in the format 'mel_<number>.npy' so importing accordingly\n",
    "        Sound_Signal = np.load(self.path + 'Mel_Embedding120/mel_' + audio_file + '.npy')\n",
    "        #mat_file = mat_file['data']\n",
    "        MEG_Signal = np.float32(MEG_Signal[:273, :])\n",
    "        #Baseline Correction\n",
    "        mean_5 = np.mean(MEG_Signal[:,:60], axis=1)\n",
    "        MEG_Signal = MEG_Signal - mean_5[:,None]\n",
    "        #Robust Scaler\n",
    "        scaler = RobustScaler().fit(MEG_Signal)\n",
    "        MEG_Signal = scaler.transform(MEG_Signal)\n",
    "        return torch.from_numpy(MEG_Signal), torch.from_numpy(Sound_Signal) , idx_file\n",
    "    else:\n",
    "        return [],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4mmK8j48ry7",
    "outputId": "76e71921-931f-435a-b030-bce832dd90e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_u9lIf3RHd-2"
   },
   "outputs": [],
   "source": [
    "#import scipy.io\n",
    "#data = scipy.io.loadmat('/content/drive/MyDrive/data.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1HN9qUTAH5Op",
    "outputId": "cd14bbbb-11a3-4807-84e4-6093c9036df3"
   },
   "outputs": [],
   "source": [
    "#x = data['data']\n",
    "#x = x[:, 1:3600:10, :]\n",
    "#print(x.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "COwyIOab8t9Q"
   },
   "outputs": [],
   "source": [
    "def cos_vector(k, K, x, y):\n",
    "  cos_v = torch.zeros(273, K)\n",
    "  for l in range(K):\n",
    "    cos_v[:,l] = torch.cos(2*math.pi*(k*x+l*y))\n",
    "  return cos_v\n",
    "def sin_vector(k, K, x, y):\n",
    "  sin_v = torch.zeros(273, K)\n",
    "  for l in range(K):\n",
    "    sin_v[:,l] = torch.sin(2*math.pi*(k*x+l*y))\n",
    "  return sin_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "shnVdPVOEPi8"
   },
   "outputs": [],
   "source": [
    "def SpatialAttentionFunc(in_channels, out_channels, X, z, K, cos, sin):\n",
    "  z_r = z.real\n",
    "  z_i = z.imag\n",
    "  a = (torch.mm(z_r.float(), torch.transpose(cos, 0, 1).float()) + torch.mm(z_i.float(), torch.transpose(sin, 0, 1).float())).to(device)\n",
    "  SA = torch.randn(out_channels, 360)\n",
    "  for j in range(out_channels):\n",
    "    exp1 = torch.mm(torch.exp(a[j, :]).unsqueeze(0), X)\n",
    "    exp2 = torch.sum(torch.exp(a[j, 0:out_channels]))\n",
    "    SA[j] = exp1/exp2\n",
    "  return SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aIPEgh2fT7hJ"
   },
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "  def __init__(self,in_channels, out_channels, K, path):\n",
    "    super(SpatialAttention, self).__init__()\n",
    "    self.out = out_channels\n",
    "    self.input = in_channels\n",
    "    self.K = K\n",
    "    self.z = Parameter(torch.randn(self.out, K*K, dtype = torch.cfloat)/(32*32))\n",
    "    self.z.requires_grad = True\n",
    "    self.positions = loadmat(path + 'electrode_positions.mat')\n",
    "    self.positions = self.positions['positions']\n",
    "    self.x = torch.tensor(self.positions[:, 0]).to(device)\n",
    "    self.y = torch.tensor(self.positions[:, 1]).to(device)\n",
    "    self.cos_v = []\n",
    "    self.sin_v = []\n",
    "    self.cos = []\n",
    "    self.sin = []\n",
    "    for i in range(in_channels):\n",
    "      self.cos_v = []\n",
    "      self.sin_v = []\n",
    "      for k in range(K):\n",
    "        for l in range(K):\n",
    "          self.cos_v.append(torch.cos(2*math.pi*(k*self.x[i]+l*self.y[i])))\n",
    "          self.sin_v.append(torch.sin(2*math.pi*(k*self.x[i]+l*self.y[i])))\n",
    "      self.cos.append(torch.stack(self.cos_v))\n",
    "      self.sin.append(torch.stack(self.sin_v))\n",
    "    self.cos = torch.stack(self.cos).to(device)\n",
    "    self.sin = torch.stack(self.sin).to(device)\n",
    "  def forward(self, X):\n",
    "    N = X.size()[0]\n",
    "    SA = torch.zeros(N, 270, 360)\n",
    "    for i in range(N):\n",
    "      SA[i] = SpatialAttentionFunc(self.input, self.out, X[i], self.z, self.K, self.cos, self.sin)\n",
    "    return SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EmBpvLtx8YwQ"
   },
   "outputs": [],
   "source": [
    "def SpatialAttentionFunc(in_channels, out_channels, X, z, K, cos, sin):\n",
    "  a = torch.randn(out_channels, in_channels).to(device)\n",
    "  #positions = loadmat('/content/drive/MyDrive/electrode_positions.mat')\n",
    "  #positions = positions['positions']\n",
    "  #x = torch.tensor(positions[:, 0])\n",
    "  #y = torch.tensor(positions[:, 1])\n",
    "  for j in range(out_channels):\n",
    "    cos_sum = torch.zeros(in_channels).to(device)\n",
    "    sin_sum = torch.zeros(in_channels).to(device)\n",
    "    for k in range(K):\n",
    "      z_r = z[j, k, :].real\n",
    "      z_r = z_r.unsqueeze(0)\n",
    "      z_i = z[j, k, :].imag\n",
    "      z_i = z_i.unsqueeze(0)      \n",
    "      #cos_k = torch.transpose(cos_vector(k, 32, x, y), 0, 1)\n",
    "      #sin_k = torch.transpose(sin_vector(k, 32, x, y), 0, 1)\n",
    "      cos_sum = cos_sum + torch.mm(z_r, cos[k])\n",
    "      sin_sum = sin_sum + torch.mm(z_i, sin[k])\n",
    "    a[j, :] = cos_sum + sin_sum \n",
    "  SA = torch.randn(out_channels, 360)\n",
    "  for j in range(out_channels):\n",
    "    exp1 = torch.mm(torch.exp(a[j, :]).unsqueeze(0), X)\n",
    "    exp2 = torch.sum(torch.exp(a[j, 0:out_channels]))\n",
    "    SA[j] = exp1/exp2\n",
    "  return SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FDzJZiF43zYh"
   },
   "outputs": [],
   "source": [
    "class SubjectLayer(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SubjectLayer, self).__init__()\n",
    "    self.layers = []\n",
    "\n",
    "    for i in range(124): #124 subjects\n",
    "      layer = nn.Conv2d(270, 270, 1)\n",
    "      self.layers.append(layer)\n",
    "      \n",
    "  def forward(self, x, s_idx):\n",
    "    for i in range(len(x)):\n",
    "      x[i] = self.layers[s_idx[i]](x[i].clone())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0_iNhxsSaGu",
    "outputId": "052e31c6-5b97-4da0-f1c3-2ba6ed555850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 270, 360, 1])\n",
      "torch.Size([3, 270, 360, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SubjectLayer()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject = SubjectLayer()\n",
    "\n",
    "x = torch.randn(3, 270, 360, 1)\n",
    "\n",
    "print(x.shape)\n",
    "output = subject(x, [0, 1, 2, 3])\n",
    "print(output.shape)\n",
    "\n",
    "subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OLOFnXXq92XT"
   },
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "  def __init__(self,in_channels, out_channels, K, path):\n",
    "    super(SpatialAttention, self).__init__()\n",
    "    self.positions = loadmat(path + 'electrode_positions.mat')\n",
    "    self.positions = self.positions['positions']\n",
    "    self.x = torch.tensor(self.positions[:, 0]).to(device)\n",
    "    self.y = torch.tensor(self.positions[:, 1]).to(device)\n",
    "    self.cos = []\n",
    "    self.sin = []\n",
    "    for k in range(32):\n",
    "      self.cos.append(torch.transpose(cos_vector(k, 32, self.x, self.y), 0, 1))\n",
    "      self.sin.append(torch.transpose(sin_vector(k, 32, self.x, self.y), 0, 1))\n",
    "    self.cos = torch.stack(self.cos).to(device)\n",
    "    self.sin = torch.stack(self.sin).to(device)\n",
    "    self.out = out_channels\n",
    "    self.input = in_channels\n",
    "    self.K = K\n",
    "    self.z = Parameter(torch.randn(out_channels, K, K, dtype = torch.cfloat)/(32*32))\n",
    "    self.z.requiresGrad = True\n",
    "  def forward(self, X):\n",
    "    N = X.size()[0]\n",
    "    SA = torch.zeros(N, 270, 360)\n",
    "    for i in range(N):\n",
    "      SA[i] = SpatialAttentionFunc(self.input, self.out, X[i], self.z, self.K, self.cos, self.sin)\n",
    "    return SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-eimM0SRDeMI"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, path):\n",
    "    super(Net, self).__init__()\n",
    "    self.SA = SpatialAttention(273, 270, 32, path)\n",
    "    self.Subject = SubjectLayer()\n",
    "  def forward(self, y, s_idx):\n",
    "    x1 = self.SA(y).unsqueeze(0)\n",
    "    x2 = torch.permute(x1, (1, 2, 3, 0)) # subject attention?\n",
    "    x3 = nn.Conv2d(270, 270, (1, 1))(x2)\n",
    "    x = self.Subject(x3, s_idx)\n",
    "    for k in range(1,6):\n",
    "      p = pow(2,(2*k)%5)\n",
    "      q = pow(2,(2*k+1)%5)\n",
    "      if k == 1:\n",
    "        x = nn.Conv2d(270, 320, (3, 1), dilation = 1, padding = (1, 0))(x)\n",
    "        x = nn.BatchNorm2d(320)(x)\n",
    "        x = nn.GELU()(x)\n",
    "        x = nn.Conv2d(320, 320, (3, 1), dilation = 1, padding = (1, 0))(x)\n",
    "        x = nn.BatchNorm2d(320)(x)\n",
    "        x = nn.GELU()(x)\n",
    "        x = nn.Conv2d(320, 640, (3, 1), dilation = 2, padding = (2, 0))(x)\n",
    "        x = torch.transpose(x, 3, 1)\n",
    "        x = nn.GLU()(x)\n",
    "        x = torch.transpose(x, 3, 1)\n",
    "      else:\n",
    "        x1 = nn.Conv2d(320, 320, (3, 1), dilation = p, padding = (p, 0))(x)\n",
    "        x1 = nn.BatchNorm2d(320)(x1)\n",
    "        x1 = nn.GELU()(x1)\n",
    "        x2 = x + x1\n",
    "        x3 = nn.Conv2d(320, 320, (3, 1), dilation = q, padding = (q, 0))(x2)\n",
    "        x3 = nn.BatchNorm2d(320)(x3)\n",
    "        x3 = nn.GELU()(x3)\n",
    "        x4 = x2 + x2\n",
    "        x_out = nn.Conv2d(320, 640, (3, 1), dilation = 2, padding = (2, 0))(x4)\n",
    "        x_out = torch.transpose(x_out, 3, 1)\n",
    "        x_out = nn.GLU()(x_out)\n",
    "        x_out = torch.transpose(x_out, 3, 1)\n",
    "    x_out = nn.Conv2d(320, 640, (1, 1))(x_out)\n",
    "    x_out = nn.GELU()(x_out)\n",
    "    x_out = nn.Conv2d(640, 120, (1, 1))(x_out)\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TRx3Md6j_YOX"
   },
   "outputs": [],
   "source": [
    "def CLIP_loss(Z, Y):\n",
    "  N = Y.size(dim = 0)\n",
    "  #inner_product = torch.zeros(N, N)\n",
    "  log_softmax = torch.zeros(N).to(device)\n",
    "  Z_row = torch.reshape(Z, (N, -1)).to(device)\n",
    "  Y_row = torch.reshape(Y, (N, -1)).to(device)\n",
    "  inner_product = (torch.mm(Z_row, torch.transpose(Y_row, 1, 0))/(N*N)).to(device)\n",
    "  for i in range(N):\n",
    "    inn = inner_product[i, :].to(device)\n",
    "    log_softmax[i] = torch.log(nn.functional.softmax(inn, -1))[i]\n",
    "  return sum(-1*log_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "eKwfFeO-FrRx"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.tracebacklimit = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 744
    },
    "id": "KJ5iCaaqHjAK",
    "outputId": "c5b8a99b-e817-4a50-ca19-9e99be22d865"
   },
   "outputs": [],
   "source": [
    "Dataset = Sound2MEGDataset(filePath)\n",
    "training_data, validation_data, test_data = random_split(Dataset, [0.7, 0.2, 0.1], generator=torch.Generator().manual_seed(42))\n",
    "Training_Data_Batches = DataLoader(training_data, batch_size = 128, shuffle = True)\n",
    "BrainModule = Net(filePath)\n",
    "BrainModule.to(device)\n",
    "optimizer = optim.Adam(BrainModule.parameters(), lr = 0.0003)\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "for i in range(1):\n",
    "  loss_t = 0\n",
    "  for MEG, WAV, Sub in Training_Data_Batches:\n",
    "    Sub = Sub.tolist()\n",
    "    Z = BrainModule(MEG.to(device), Sub)\n",
    "    Z = Z[:, :, :, 0]\n",
    "    WAV.to(device)\n",
    "    loss = CLIP_loss(Z, WAV.abs().to(device))\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    print(loss.item())\n",
    "    loss_t = loss_t + loss.item()\n",
    "    optimizer.step()\n",
    "  loss_train.append(loss_t/len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "h-1bxwnlQLQS",
    "outputId": "d23adab4-97b2-4a4f-f876-a387befb31bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "AssertionError\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "MEG_val = []\n",
    "WAV_val = []\n",
    "Sub_val = []\n",
    "for i in range(len(validation_data)):\n",
    "  MEG_val.append(validation_data[i][0])\n",
    "  WAV_val.append(validation_data[i][1])\n",
    "  Sub_val.append(validation_data[i][2])\n",
    "MEG_val = torch.stack(MEG_val)\n",
    "WAV_val = torch.stack(WAV_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Sound2MEGDataset at 0x15548d028190>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = np.array(loadmat( filePath + 'file_sizes.mat')['file_sizes'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([105, 174, 169])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes[61:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "sizes = np.empty(126)\n",
    "for subject in range(0, 126):\n",
    "    if os.path.exists(filePath + 'MEG_Signals/S%03dT000.npy'%subject):\n",
    "        files = glob.glob(filePath + 'MEG_Signals/S%03dT*.npy'%subject)\n",
    "        sizes[subject] = len(files)\n",
    "    else:\n",
    "        sizes[subject] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0., 147., 168., 171., 166., 167., 178., 170., 152., 180.,\n",
       "        99.,   0., 165., 169., 170., 169., 170.,   0., 171., 164., 174.,\n",
       "         0.,   0., 160., 174.,   0., 183., 171., 178., 162., 155., 167.,\n",
       "       163., 154., 164.,  90., 174., 168., 162., 173., 163., 170.,   0.,\n",
       "         0.,   0., 170., 169.,   0., 170., 178., 174., 170., 163.,   0.,\n",
       "       162., 164., 170., 179., 164.,   0., 178., 146.,   0., 177., 170.,\n",
       "       160., 169., 178., 153., 166., 172., 158., 167.,   0., 164., 105.,\n",
       "         0., 174., 169., 169.,   0.,   0., 166.,  49., 172., 168.,   0.,\n",
       "       164., 166., 166., 165., 177.,   0., 161., 166., 169., 168., 178.,\n",
       "       165.,   0., 168., 160., 171., 171., 169., 165.,   0., 173., 157.,\n",
       "       170., 171.,   0., 157., 171.,   0., 171., 172.,   0., 156., 175.,\n",
       "       158., 168.,   0., 168., 164.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes = np.empty(0)\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = Sound2MEGDataset(filePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([147., 168., 171., 166., 167., 178., 170., 152., 180.,  99., 165.,\n",
       "       169., 170., 169., 170., 171., 164., 174., 160., 174., 183., 171.,\n",
       "       178., 162., 155., 167., 163., 154., 164.,  90., 174., 168., 162.,\n",
       "       173., 163., 170., 170., 169., 170., 178., 174., 170., 163., 162.,\n",
       "       164., 170., 179., 164., 178., 146., 177., 170., 160., 169., 178.,\n",
       "       153., 166., 172., 158., 167., 164., 105., 174., 169., 169., 166.,\n",
       "        49., 172., 168., 164., 166., 166., 165., 177., 161., 166., 169.,\n",
       "       168., 178., 165., 168., 160., 171., 171., 169., 165., 173., 157.,\n",
       "       170., 171., 157., 171., 171., 172., 156., 175., 158., 168., 168.,\n",
       "       164.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0+cu117'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
