{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sccn/sound2meg/blob/main/Spatial_Attention_Tae_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvwsUf4jh2bC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.optim as optim\n",
        "from scipy.io import loadmat\n",
        "from dataset_loading import Sound2MEGDataset\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "l4mmK8j48ry7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.tracebacklimit = 0"
      ],
      "metadata": {
        "id": "bLPOBgJHzd2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialAttention(nn.Module):\n",
        "  def __init__(self,in_channels, out_channels, K, path):\n",
        "    super(SpatialAttention, self).__init__()\n",
        "    self.out = out_channels\n",
        "    self.input = in_channels\n",
        "    self.K = K\n",
        "    self.z = Parameter(torch.randn(self.out, K*K, dtype = torch.cfloat)/(32*32))\n",
        "    self.z.requires_grad = True\n",
        "    self.positions = loadmat(path + 'electrode_positions.mat')\n",
        "    self.positions = self.positions['positions']\n",
        "    self.x = torch.tensor(self.positions[:, 0]).to(device)\n",
        "    self.y = torch.tensor(self.positions[:, 1]).to(device)\n",
        "    self.cos = []\n",
        "    self.sin = []\n",
        "    for i in range(in_channels):\n",
        "      self.cos_v = []\n",
        "      self.sin_v = []\n",
        "      for k in range(K):\n",
        "        for l in range(K):\n",
        "          self.cos_v.append(torch.cos(2*math.pi*(k*self.x[i]+l*self.y[i])))\n",
        "          self.sin_v.append(torch.sin(2*math.pi*(k*self.x[i]+l*self.y[i])))\n",
        "      self.cos.append(torch.stack(self.cos_v))\n",
        "      self.sin.append(torch.stack(self.sin_v))\n",
        "    self.cos = torch.stack(self.cos).to(device)\n",
        "    self.sin = torch.stack(self.sin).to(device)\n",
        "  def forward(self, X):\n",
        "    N = X.size()[0]\n",
        "    SA = torch.zeros(N, 270, 360).to(device)\n",
        "    z_r = self.z.real\n",
        "    z_i = self.z.imag\n",
        "    a = (torch.mm(z_r.float(), torch.transpose(self.cos, 0, 1).float()) + torch.mm(z_i.float(), torch.transpose(self.sin, 0, 1).float())).to(device)\n",
        "    exp2 = torch.sum(torch.exp(a[:, 0:self.out]), 1).to(device)\n",
        "    exp2 = torch.transpose(exp2.unsqueeze(0), 0, 1)\n",
        "    exp2 = torch.mm(exp2, torch.ones(1, 360).to(device))\n",
        "    for i in range(N):\n",
        "      exp1 = torch.mm(torch.exp(a), X[i]).to(device)\n",
        "      SA[i] = (exp1/exp2).to(device)\n",
        "      #SA[i] = SpatialAttentionSoftmax(self.input, self.out, X[i], a)\n",
        "    return SA"
      ],
      "metadata": {
        "id": "aIPEgh2fT7hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SubjectLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SubjectLayer, self).__init__()\n",
        "    self.layers = []\n",
        "\n",
        "    for i in range(124): #124 subjects\n",
        "      layer = nn.Conv2d(270, 270, 1).to(device)\n",
        "      self.layers.append(layer)\n",
        "      \n",
        "  def forward(self, x, s_idx):\n",
        "    x = x.unsqueeze(1)\n",
        "    for i in range(len(x)):\n",
        "      x[i] = self.layers[s_idx[i]](x[i].clone())\n",
        "    return x[:, 0, :, :]"
      ],
      "metadata": {
        "id": "FDzJZiF43zYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eimM0SRDeMI"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, path, F):\n",
        "    super(Net, self).__init__()\n",
        "    self.SA = SpatialAttention(273, 270, 32, path)\n",
        "    self.Subject = SubjectLayer().to(device)\n",
        "    self.F = F\n",
        "  def forward(self, x, s_idx):\n",
        "    x = self.SA(x).unsqueeze(0)\n",
        "    x = torch.permute(x, (1, 2, 3, 0)) # subject attention?\n",
        "    x = nn.Conv2d(270, 270, (1, 1)).to(device)(x)\n",
        "    x = self.Subject(x, s_idx)\n",
        "    for k in range(1,6):\n",
        "      p = pow(2,(2*k)%5)\n",
        "      q = pow(2,(2*k+1)%5)\n",
        "      if k == 1:\n",
        "        x = nn.Conv2d(270, 320, (3, 1), dilation = 1, padding = (1, 0)).to(device)(x)\n",
        "        x = nn.BatchNorm2d(320).to(device)(x)\n",
        "        x = nn.GELU().to(device)(x)\n",
        "        x = nn.Conv2d(320, 320, (3, 1), dilation = 1, padding = (1, 0)).to(device)(x)\n",
        "        x = nn.BatchNorm2d(320).to(device)(x)\n",
        "        x = nn.GELU().to(device)(x)\n",
        "        x = nn.Conv2d(320, 640, (3, 1), dilation = 2, padding = (2, 0)).to(device)(x)\n",
        "        x = torch.transpose(x, 3, 1)\n",
        "        x = nn.GLU().to(device)(x)\n",
        "        x = torch.transpose(x, 3, 1)\n",
        "      else:\n",
        "        x1 = nn.Conv2d(320, 320, (3, 1), dilation = p, padding = (p, 0)).to(device)(x)\n",
        "        x1 = nn.BatchNorm2d(320).to(device)(x1)\n",
        "        x1 = nn.GELU().to(device)(x1)\n",
        "        x2 = x + x1\n",
        "        x3 = nn.Conv2d(320, 320, (3, 1), dilation = q, padding = (q, 0)).to(device)(x2)\n",
        "        x3 = nn.BatchNorm2d(320).to(device)(x3)\n",
        "        x3 = nn.GELU().to(device)(x3)\n",
        "        x4 = x2 + x2\n",
        "        x_out = nn.Conv2d(320, 640, (3, 1), dilation = 2, padding = (2, 0)).to(device)(x4)\n",
        "        x_out = torch.transpose(x_out, 3, 1)\n",
        "        x_out = nn.GLU().to(device)(x_out)\n",
        "        x_out = torch.transpose(x_out, 3, 1)\n",
        "    x_out = nn.Conv2d(320, 640, (1, 1)).to(device)(x_out)\n",
        "    x_out = nn.GELU().to(device)(x_out)\n",
        "    x_out = nn.Conv2d(640, self.F, (1, 1)).to(device)(x_out)\n",
        "    return x_out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, path, F):\n",
        "    super(Net, self).__init__()\n",
        "    self.SA = SpatialAttention(273, 270, 32, path)\n",
        "    self.Subject = SubjectLayer().to(device)\n",
        "    self.F = F\n",
        "    self.conv1 = nn.Conv2d(270, 270, (1, 1)).to(device)\n",
        "    self.conv2 = nn.Conv2d(320, 640, (1, 1)).to(device)\n",
        "    self.conv3 = nn.Conv2d(640, self.F, (1, 1)).to(device)\n",
        "    self.gelu = nn.GELU().to(device)\n",
        "    self.loop_convs = []\n",
        "    self.loop_batchnorms = []\n",
        "    self.loop_gelus = []\n",
        "    self.loop_glus = []\n",
        "    for k in range(1, 6):\n",
        "      p = pow(2,(2*k)%5)\n",
        "      q = pow(2,(2*k+1)%5)\n",
        "      self.convs = []\n",
        "      self.batchnorms = []\n",
        "      self.gelus = []\n",
        "      self.convs.append(nn.Conv2d(320, 320, (3, 1), dilation = p, padding = (p, 0)).to(device))\n",
        "      self.convs.append(nn.Conv2d(320, 320, (3, 1), dilation = q, padding = (q, 0)).to(device))\n",
        "      self.convs.append(nn.Conv2d(320, 640, (3, 1), dilation = 2, padding = (2, 0)).to(device))\n",
        "      for i in range(2):\n",
        "        self.batchnorms.append(nn.BatchNorm2d(320).to(device))\n",
        "        self.gelus.append(nn.GELU().to(device))\n",
        "      self.loop_convs.append(self.convs)\n",
        "      self.loop_batchnorms.append(self.batchnorms)\n",
        "      self.loop_gelus.append(self.gelus)\n",
        "      self.loop_glus.append(nn.GLU().to(device))\n",
        "    self.loop_convs[0][0] = nn.Conv2d(270, 320, (3, 1), dilation = 1, padding = (1, 0)).to(device)\n",
        "  def forward(self, x, s_idx):\n",
        "    x = self.SA(x).unsqueeze(3)\n",
        "    x = self.conv1(x)\n",
        "    x = self.Subject(x, s_idx)\n",
        "    for i in range(5):\n",
        "      if i == 0:\n",
        "        x = self.loop_convs[0][0](x)\n",
        "        x = self.loop_batchnorms[0][0](x)\n",
        "        x = self.loop_gelus[0][0](x)\n",
        "        x = self.loop_convs[0][1](x)\n",
        "        x = self.loop_batchnorms[0][1](x)\n",
        "        x = self.loop_gelus[0][1](x)\n",
        "        x = self.loop_convs[0][2](x)\n",
        "        x = torch.transpose(x, 3, 1)\n",
        "        x = self.loop_glus[0](x)\n",
        "        x = torch.transpose(x, 3, 1)\n",
        "      else:\n",
        "        x1 = self.loop_convs[i][0](x)\n",
        "        x1 = self.loop_batchnorms[i][0](x)\n",
        "        x1 = self.loop_gelus[i][0](x)\n",
        "        x2 = x + x1\n",
        "        x3 = self.loop_convs[i][1](x2)\n",
        "        x3 = self.loop_batchnorms[i][1](x3)\n",
        "        x3 = self.loop_gelus[i][1](x3)\n",
        "        x4 = x2 + x3\n",
        "        x5 = self.loop_convs[i][2](x4)\n",
        "        x5 = torch.transpose(x5, 3, 1)\n",
        "        x5 = self.loop_glus[i](x5)\n",
        "        x = torch.transpose(x5, 3, 1)\n",
        "    x_out = self.conv2(x)\n",
        "    x_out = self.gelu(x_out)\n",
        "    x_out = self.conv3(x_out)\n",
        "    return x_out"
      ],
      "metadata": {
        "id": "GMhugi2l6U-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CLIP_loss(Z, Y):\n",
        "  N = Y.size(dim = 0)\n",
        "  #inner_product = torch.zeros(N, N)\n",
        "  log_softmax = torch.zeros(N).to(device)\n",
        "  Z_row = torch.reshape(Z, (N, -1)).to(device)\n",
        "  Y_row = torch.reshape(Y, (N, -1)).to(device)\n",
        "  inner_product = (torch.mm(Z_row, torch.transpose(Y_row, 1, 0))/(N*N)).to(device)\n",
        "  for i in range(N):\n",
        "    inn = inner_product[i, :].to(device)\n",
        "    log_softmax[i] = torch.log(nn.functional.softmax(inn, -1))[i]\n",
        "  return sum(-1*log_softmax)"
      ],
      "metadata": {
        "id": "TRx3Md6j_YOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = Sound2MEGDataset('/content/drive/MyDrive/sound2meg/')\n",
        "training_data, validation_data, test_data = random_split(Dataset, [11497, 3285, 1642], generator=torch.Generator().manual_seed(42))\n",
        "Training_Data_Batches = DataLoader(training_data, batch_size = 2, shuffle = True)\n",
        "Validation_Data_Batches = DataLoader(validation_data, batch_size = 128, shuffle = True)\n",
        "BrainModule = Net('/content/drive/MyDrive/sound2meg/', 120)\n",
        "BrainModule.to(device)\n",
        "optimizer = optim.Adam(BrainModule.parameters(), lr = 0.0003)\n",
        "loss_train = []\n",
        "loss_val = []\n",
        "\n",
        "for i in range(1):\n",
        "  loss_t = 0\n",
        "  loss_v = 0\n",
        "  for MEG, WAV, Sub in Training_Data_Batches:\n",
        "    Sub = Sub.tolist()\n",
        "    Z = BrainModule(MEG.to(device), Sub)\n",
        "    Z = Z[:, :, :, 0]\n",
        "    loss = CLIP_loss(Z.float(), WAV.abs().float().to(device))\n",
        "    print(loss.item())\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    loss_t = loss_t + loss.item()\n",
        "    optimizer.step()\n",
        "  print(loss_t/len(Training_Data_Batches))\n",
        "  loss_train.append(loss_t/len(Training_Data_Batches))\n",
        "  for MEG_val, WAV_val, Sub_val in Validation_Data_Batches:\n",
        "    Z_val = BrainModule(MEG_val.to(device), Sub_val)\n",
        "    loss = CLIP_loss(Z_val.float(), WAV_val.abs().float().to(device))\n",
        "    print(loss.item())\n",
        "    loss_v = loss_v + loss.item()\n",
        "  print(loss_v/len(Validation_Data_Batches))\n",
        "  loss_val.append(loss_v/len(Validation_Data_Batches))\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "print(loss_train)\n",
        "print(loss_val)"
      ],
      "metadata": {
        "id": "KJ5iCaaqHjAK",
        "outputId": "de27029e-62aa-4814-e2a8-0e7c101fd27d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.3873982429504395\n",
            "1.3882249593734741\n",
            "1.3879494667053223\n",
            "1.3862934112548828\n",
            "1.389387607574463\n",
            "1.384629249572754\n",
            "1.3872852325439453\n",
            "1.3856542110443115\n",
            "1.384668231010437\n",
            "1.3859007358551025\n",
            "1.3880372047424316\n",
            "1.3868567943572998\n",
            "1.3912134170532227\n",
            "1.3903553485870361\n",
            "1.3858633041381836\n",
            "1.3876519203186035\n",
            "1.387650966644287\n",
            "1.3971385955810547\n",
            "1.3815388679504395\n",
            "1.3769750595092773\n",
            "1.387977123260498\n",
            "1.3864128589630127\n",
            "1.3916904926300049\n",
            "1.3902852535247803\n",
            "1.3709532022476196\n",
            "1.4081848859786987\n",
            "1.3905868530273438\n",
            "1.381545066833496\n",
            "1.3818318843841553\n",
            "1.3849722146987915\n",
            "1.383784294128418\n",
            "1.384390950202942\n",
            "1.3874726295471191\n",
            "1.385755181312561\n",
            "1.3827954530715942\n",
            "1.3842144012451172\n",
            "1.3814703226089478\n",
            "1.3906269073486328\n",
            "1.3754392862319946\n",
            "1.38837730884552\n",
            "1.4013357162475586\n",
            "1.3717029094696045\n",
            "1.3822886943817139\n",
            "1.393576741218567\n",
            "1.3816211223602295\n",
            "1.3862991333007812\n",
            "1.3889961242675781\n",
            "1.4062047004699707\n",
            "1.409177303314209\n",
            "1.389685869216919\n",
            "1.3814055919647217\n",
            "1.3753578662872314\n",
            "1.3895330429077148\n",
            "1.3952629566192627\n",
            "1.3832422494888306\n",
            "1.3948886394500732\n",
            "1.39398193359375\n",
            "1.382173776626587\n",
            "1.3766303062438965\n",
            "1.3895155191421509\n",
            "1.3824753761291504\n",
            "1.3784332275390625\n",
            "1.3910794258117676\n",
            "1.3872660398483276\n",
            "1.3862943649291992\n",
            "1.388993263244629\n",
            "1.3857159614562988\n",
            "1.3890273571014404\n",
            "1.3785452842712402\n",
            "1.3883525133132935\n",
            "1.382704496383667\n",
            "1.3775956630706787\n",
            "1.3823261260986328\n",
            "1.3965590000152588\n",
            "1.3744184970855713\n",
            "1.369059681892395\n",
            "1.3984956741333008\n",
            "1.3740453720092773\n",
            "1.3807756900787354\n",
            "1.3653953075408936\n",
            "1.3966028690338135\n",
            "1.3760393857955933\n",
            "1.3876519203186035\n",
            "1.3768863677978516\n",
            "1.3776297569274902\n",
            "1.3792474269866943\n",
            "1.384246587753296\n",
            "1.3914413452148438\n",
            "1.4112470149993896\n",
            "1.3845605850219727\n",
            "1.3884024620056152\n",
            "1.383792757987976\n",
            "1.3736692667007446\n",
            "1.4339743852615356\n",
            "1.4059332609176636\n",
            "1.3885900974273682\n",
            "1.3888028860092163\n",
            "1.3868319988250732\n",
            "1.372694492340088\n",
            "1.3839696645736694\n",
            "1.4013481140136719\n",
            "1.3686131238937378\n",
            "1.3740931749343872\n",
            "1.386573076248169\n",
            "1.3884248733520508\n",
            "1.402616262435913\n",
            "1.3544666767120361\n",
            "1.3836781978607178\n",
            "1.4290672540664673\n",
            "1.3910565376281738\n",
            "1.3918771743774414\n",
            "1.382089614868164\n",
            "1.4048165082931519\n",
            "1.378373146057129\n",
            "1.3933298587799072\n",
            "1.3790557384490967\n",
            "1.4578291177749634\n",
            "1.3973464965820312\n",
            "1.3726344108581543\n",
            "1.3624542951583862\n",
            "1.3917052745819092\n",
            "1.3821815252304077\n",
            "1.3908519744873047\n",
            "1.3431944847106934\n",
            "1.3926466703414917\n",
            "1.3748550415039062\n",
            "1.368314504623413\n",
            "1.371736764907837\n",
            "1.3962271213531494\n",
            "1.3813475370407104\n",
            "1.4028172492980957\n",
            "1.3839783668518066\n",
            "1.4207602739334106\n",
            "1.3572179079055786\n",
            "1.3833690881729126\n",
            "1.386192798614502\n",
            "1.3827028274536133\n",
            "1.3815975189208984\n",
            "1.3933281898498535\n",
            "1.3945341110229492\n",
            "1.4335219860076904\n",
            "1.380365252494812\n",
            "1.3871846199035645\n",
            "1.384342908859253\n",
            "1.3680922985076904\n",
            "1.3926265239715576\n",
            "1.3857866525650024\n",
            "1.405776858329773\n",
            "1.3629817962646484\n",
            "1.3994325399398804\n",
            "1.3898253440856934\n",
            "1.360392451286316\n",
            "1.4037601947784424\n",
            "1.3678706884384155\n",
            "1.3699828386306763\n",
            "1.375852346420288\n",
            "1.399080514907837\n",
            "1.3940954208374023\n",
            "1.3643040657043457\n",
            "1.4002330303192139\n",
            "1.392777919769287\n",
            "1.3788368701934814\n",
            "1.4040628671646118\n",
            "1.3966994285583496\n",
            "1.383744716644287\n",
            "1.4003174304962158\n",
            "1.3917303085327148\n",
            "1.4313195943832397\n",
            "1.3824880123138428\n",
            "1.408432960510254\n",
            "1.3827083110809326\n",
            "1.3793315887451172\n",
            "1.3842661380767822\n",
            "1.3749078512191772\n",
            "1.3746695518493652\n",
            "1.3856689929962158\n",
            "1.4026774168014526\n",
            "1.400036334991455\n",
            "1.3465723991394043\n",
            "1.3846468925476074\n",
            "1.3828009366989136\n",
            "1.3888428211212158\n",
            "1.4102838039398193\n",
            "1.383949637413025\n",
            "1.375637412071228\n",
            "1.386696457862854\n",
            "1.3935644626617432\n",
            "1.3664190769195557\n",
            "1.398983359336853\n",
            "1.3816181421279907\n",
            "1.3857808113098145\n",
            "1.3813936710357666\n",
            "1.4060797691345215\n",
            "1.3897842168807983\n",
            "1.3818166255950928\n",
            "1.4011930227279663\n",
            "1.3901398181915283\n",
            "1.3860031366348267\n",
            "1.3840748071670532\n",
            "1.3895665407180786\n",
            "1.373544692993164\n",
            "1.3856329917907715\n",
            "1.376016616821289\n",
            "1.3943730592727661\n",
            "1.3927618265151978\n",
            "1.3678874969482422\n",
            "1.3724455833435059\n",
            "1.3884007930755615\n",
            "1.394651174545288\n",
            "1.3848764896392822\n",
            "1.3740605115890503\n",
            "1.3926596641540527\n",
            "1.3745436668395996\n",
            "1.4091861248016357\n",
            "1.393324613571167\n",
            "1.3401882648468018\n",
            "1.3590131998062134\n",
            "1.3918108940124512\n",
            "1.3912241458892822\n",
            "1.3862383365631104\n",
            "1.3895680904388428\n",
            "1.3814013004302979\n",
            "1.3744044303894043\n",
            "1.3742525577545166\n",
            "1.3698382377624512\n",
            "1.3826589584350586\n",
            "1.4030593633651733\n",
            "1.3980894088745117\n",
            "1.387391209602356\n",
            "1.3598580360412598\n",
            "1.3817129135131836\n",
            "1.3938570022583008\n",
            "1.3842153549194336\n",
            "1.4374994039535522\n",
            "1.3181579113006592\n",
            "1.3510262966156006\n",
            "1.406121015548706\n",
            "1.3916715383529663\n",
            "1.3817920684814453\n",
            "1.3836991786956787\n",
            "1.3729588985443115\n",
            "1.39768648147583\n",
            "1.396243691444397\n",
            "1.3982954025268555\n",
            "1.3963468074798584\n",
            "1.3823623657226562\n",
            "1.3822417259216309\n",
            "1.363667607307434\n",
            "1.41804039478302\n",
            "1.383256435394287\n",
            "1.3845584392547607\n",
            "1.3982737064361572\n",
            "1.3877956867218018\n",
            "1.3810980319976807\n",
            "1.3834681510925293\n",
            "1.3716038465499878\n",
            "1.3848493099212646\n",
            "1.4087990522384644\n",
            "1.4088953733444214\n",
            "1.4122068881988525\n",
            "1.3537225723266602\n",
            "1.395369291305542\n",
            "1.401042103767395\n",
            "1.3737341165542603\n",
            "1.3984034061431885\n",
            "1.3962059020996094\n",
            "1.3730244636535645\n",
            "1.3930768966674805\n",
            "1.3792635202407837\n",
            "1.3723080158233643\n",
            "1.3587212562561035\n",
            "1.392610788345337\n",
            "1.3843553066253662\n",
            "1.4502298831939697\n",
            "1.4285502433776855\n",
            "1.3573181629180908\n",
            "1.4149501323699951\n",
            "1.379279375076294\n",
            "1.3698418140411377\n",
            "1.3657870292663574\n",
            "1.4157885313034058\n",
            "1.4081568717956543\n",
            "1.3763459920883179\n",
            "1.4527769088745117\n",
            "1.3847947120666504\n",
            "1.4081521034240723\n",
            "1.3899332284927368\n",
            "1.3946340084075928\n",
            "1.3751795291900635\n",
            "1.36671781539917\n",
            "1.4046761989593506\n",
            "1.3614447116851807\n",
            "1.3592437505722046\n",
            "1.4033188819885254\n",
            "1.4039348363876343\n",
            "1.3976099491119385\n",
            "1.4040555953979492\n",
            "1.412722110748291\n",
            "1.3854711055755615\n",
            "1.3901302814483643\n",
            "1.3785908222198486\n",
            "1.4004430770874023\n",
            "1.3652945756912231\n",
            "1.4035813808441162\n",
            "1.3990398645401\n",
            "1.4382328987121582\n",
            "1.3977560997009277\n",
            "1.4287779331207275\n",
            "1.4001288414001465\n",
            "1.3623158931732178\n",
            "1.3959531784057617\n",
            "1.3840019702911377\n",
            "1.3683685064315796\n",
            "1.3880832195281982\n",
            "1.3567312955856323\n",
            "1.3970229625701904\n",
            "1.3939881324768066\n",
            "1.3811521530151367\n",
            "1.3650156259536743\n",
            "1.3594419956207275\n",
            "1.3916077613830566\n",
            "1.397409439086914\n",
            "1.3841161727905273\n",
            "1.3966870307922363\n",
            "1.3574426174163818\n",
            "1.398576021194458\n",
            "1.394484519958496\n",
            "1.385826826095581\n",
            "1.3815138339996338\n",
            "1.3317922353744507\n",
            "1.3863143920898438\n",
            "1.407132625579834\n",
            "1.392207384109497\n",
            "1.3859355449676514\n",
            "1.3612940311431885\n",
            "1.3875370025634766\n",
            "1.3686050176620483\n",
            "1.4137051105499268\n",
            "1.3681260347366333\n",
            "1.4038009643554688\n",
            "1.3970873355865479\n",
            "1.3770246505737305\n",
            "1.3917646408081055\n",
            "1.3691802024841309\n",
            "1.3899465799331665\n",
            "1.378704309463501\n",
            "1.4130933284759521\n",
            "1.3940651416778564\n",
            "1.4103342294692993\n",
            "1.3790452480316162\n",
            "1.3903124332427979\n",
            "1.3848381042480469\n",
            "1.4249868392944336\n",
            "1.362048864364624\n",
            "1.4003641605377197\n",
            "1.4041895866394043\n",
            "1.3859515190124512\n",
            "1.3804023265838623\n",
            "1.4101660251617432\n",
            "1.390929937362671\n",
            "1.3851087093353271\n",
            "1.3881564140319824\n",
            "1.3825677633285522\n",
            "1.3841209411621094\n",
            "1.3818150758743286\n",
            "1.3912460803985596\n",
            "1.3769006729125977\n",
            "1.3811357021331787\n",
            "1.387569785118103\n",
            "1.3742938041687012\n",
            "1.4153728485107422\n",
            "1.4035133123397827\n",
            "1.3921774625778198\n",
            "1.3577897548675537\n",
            "1.390373945236206\n",
            "1.4163212776184082\n",
            "1.41605806350708\n",
            "1.4132952690124512\n",
            "1.369663953781128\n",
            "1.3916804790496826\n",
            "1.407325267791748\n",
            "1.390455961227417\n",
            "1.399524211883545\n",
            "1.3727738857269287\n",
            "1.3826956748962402\n",
            "1.3933994770050049\n",
            "1.3821418285369873\n",
            "1.3982843160629272\n",
            "1.4006752967834473\n",
            "1.4031836986541748\n",
            "1.3707956075668335\n",
            "1.3993971347808838\n",
            "1.3780982494354248\n",
            "1.3986835479736328\n",
            "1.3865089416503906\n",
            "1.3820478916168213\n",
            "1.3940635919570923\n",
            "1.3632285594940186\n",
            "1.3737382888793945\n",
            "1.391357660293579\n",
            "1.3864363431930542\n",
            "1.3847750425338745\n",
            "1.382894515991211\n",
            "1.3817598819732666\n",
            "1.3979336023330688\n",
            "1.3838212490081787\n",
            "1.379176378250122\n",
            "1.3846136331558228\n",
            "1.3978469371795654\n",
            "1.394303321838379\n",
            "1.3860805034637451\n",
            "1.3923957347869873\n",
            "1.4068478345870972\n",
            "1.3831197023391724\n",
            "1.3934717178344727\n",
            "1.3961975574493408\n",
            "1.3820011615753174\n",
            "1.3891994953155518\n",
            "1.380464792251587\n",
            "1.3884567022323608\n",
            "1.3762681484222412\n",
            "1.3828175067901611\n",
            "1.3718748092651367\n",
            "1.3764090538024902\n",
            "1.3962520360946655\n",
            "1.3838719129562378\n",
            "1.3773744106292725\n",
            "1.3970686197280884\n",
            "1.4074939489364624\n",
            "1.386439323425293\n",
            "1.3982055187225342\n",
            "1.3757810592651367\n",
            "1.3735712766647339\n",
            "1.3588712215423584\n",
            "1.3976162672042847\n",
            "1.3616199493408203\n",
            "1.4040734767913818\n",
            "1.3624904155731201\n",
            "1.394592523574829\n",
            "1.3902842998504639\n",
            "1.3877339363098145\n",
            "1.3629937171936035\n",
            "1.3912591934204102\n",
            "1.3857882022857666\n",
            "1.420756220817566\n",
            "1.3833074569702148\n",
            "1.379143476486206\n",
            "1.415128469467163\n",
            "1.364651083946228\n",
            "1.373201847076416\n",
            "1.3692054748535156\n",
            "1.3920941352844238\n",
            "1.386573314666748\n",
            "1.408521294593811\n",
            "1.4075759649276733\n",
            "1.302619457244873\n",
            "1.3729619979858398\n",
            "1.4333109855651855\n",
            "1.4065032005310059\n",
            "1.380188226699829\n",
            "1.4029991626739502\n",
            "1.3526654243469238\n",
            "1.4132781028747559\n",
            "1.390386939048767\n",
            "1.3849972486495972\n",
            "1.3799519538879395\n",
            "1.3688271045684814\n",
            "1.3766186237335205\n",
            "1.385502815246582\n",
            "1.3897120952606201\n",
            "1.40250563621521\n",
            "1.3994274139404297\n",
            "1.3892842531204224\n",
            "1.3749182224273682\n",
            "1.376665711402893\n",
            "1.3787086009979248\n",
            "1.358229637145996\n",
            "1.380179762840271\n",
            "1.4041423797607422\n",
            "1.3711512088775635\n",
            "1.380955457687378\n",
            "1.3957499265670776\n",
            "1.359879970550537\n",
            "1.4308115243911743\n",
            "1.4153118133544922\n",
            "1.3885996341705322\n",
            "1.3758654594421387\n",
            "1.3896024227142334\n",
            "1.3795416355133057\n",
            "1.3969390392303467\n",
            "1.3843843936920166\n",
            "1.4040412902832031\n",
            "1.3928802013397217\n",
            "1.389860987663269\n",
            "1.3880350589752197\n",
            "1.3855938911437988\n",
            "1.3640203475952148\n",
            "1.3837450742721558\n",
            "1.3828415870666504\n",
            "1.4157798290252686\n",
            "1.3810749053955078\n",
            "1.3790504932403564\n",
            "1.3509347438812256\n",
            "1.4020349979400635\n",
            "1.3728046417236328\n",
            "1.359342336654663\n",
            "1.409796953201294\n",
            "1.4007418155670166\n",
            "1.3970439434051514\n",
            "1.3948371410369873\n",
            "1.4119443893432617\n",
            "1.3879120349884033\n",
            "1.39775550365448\n",
            "1.37324059009552\n",
            "1.4304611682891846\n",
            "1.4199917316436768\n",
            "1.3767726421356201\n",
            "1.3893675804138184\n",
            "1.3768863677978516\n",
            "1.3876012563705444\n",
            "1.406996726989746\n",
            "1.363508701324463\n",
            "1.3762362003326416\n",
            "1.3696433305740356\n",
            "1.3863391876220703\n",
            "1.3796708583831787\n",
            "1.373281717300415\n",
            "1.42478346824646\n",
            "1.3830177783966064\n",
            "1.391953706741333\n",
            "1.3848354816436768\n",
            "1.3702316284179688\n",
            "1.3854279518127441\n",
            "1.4132205247879028\n",
            "1.4468101263046265\n",
            "1.390676736831665\n",
            "1.3900682926177979\n",
            "1.3739228248596191\n",
            "1.3764431476593018\n",
            "1.351800799369812\n",
            "1.3944790363311768\n",
            "1.3695833683013916\n",
            "1.3781079053878784\n",
            "1.3843379020690918\n",
            "1.4053940773010254\n",
            "1.3924522399902344\n",
            "1.403879165649414\n",
            "1.3917876482009888\n",
            "1.3857297897338867\n",
            "1.396820068359375\n",
            "1.3959977626800537\n",
            "1.3582282066345215\n",
            "1.387307047843933\n",
            "1.3717012405395508\n",
            "1.3723232746124268\n",
            "1.3652219772338867\n",
            "1.3458504676818848\n",
            "1.3854527473449707\n",
            "1.3777720928192139\n",
            "1.3661644458770752\n",
            "1.387507677078247\n",
            "1.3797283172607422\n",
            "1.3839205503463745\n",
            "1.3782458305358887\n",
            "1.3778910636901855\n",
            "1.3695950508117676\n",
            "1.408754587173462\n",
            "1.3610643148422241\n",
            "1.396968126296997\n",
            "1.3984019756317139\n",
            "1.3769290447235107\n",
            "1.397074580192566\n",
            "1.4327797889709473\n",
            "1.4101247787475586\n",
            "1.3818265199661255\n",
            "1.3838140964508057\n",
            "1.3812310695648193\n",
            "1.3996602296829224\n",
            "1.4013794660568237\n",
            "1.3663876056671143\n",
            "1.4018876552581787\n",
            "1.392787218093872\n",
            "1.4138286113739014\n",
            "1.34041428565979\n",
            "1.388920545578003\n",
            "1.3689507246017456\n",
            "1.3610613346099854\n",
            "1.391859531402588\n",
            "1.4026877880096436\n",
            "1.389560341835022\n",
            "1.31318199634552\n",
            "1.4152823686599731\n",
            "1.384232521057129\n",
            "1.4118432998657227\n",
            "1.3637229204177856\n",
            "1.4170451164245605\n",
            "1.3738808631896973\n",
            "1.392040729522705\n",
            "1.3939517736434937\n",
            "1.3689601421356201\n",
            "1.3758630752563477\n",
            "1.380530595779419\n",
            "1.3837509155273438\n",
            "1.376697063446045\n",
            "1.3952219486236572\n",
            "1.3804676532745361\n",
            "1.3892481327056885\n",
            "1.3714015483856201\n",
            "1.405491590499878\n",
            "1.4429367780685425\n",
            "1.4011086225509644\n",
            "1.4436979293823242\n",
            "1.3955825567245483\n",
            "1.3619893789291382\n",
            "1.3860063552856445\n",
            "1.3843518495559692\n",
            "1.3590763807296753\n",
            "1.3422895669937134\n",
            "1.4052103757858276\n",
            "1.3967926502227783\n",
            "1.3699078559875488\n",
            "1.392148494720459\n",
            "1.3087563514709473\n",
            "1.3920371532440186\n",
            "1.381795883178711\n",
            "1.3899967670440674\n",
            "1.3620591163635254\n",
            "1.3775136470794678\n",
            "1.3146440982818604\n",
            "1.4300425052642822\n",
            "1.400695562362671\n",
            "1.3902075290679932\n",
            "1.3354218006134033\n",
            "1.4040110111236572\n",
            "1.3957889080047607\n",
            "1.373469591140747\n",
            "1.3804686069488525\n",
            "1.402611255645752\n",
            "1.4197533130645752\n",
            "1.3768022060394287\n",
            "1.3697631359100342\n",
            "1.3633029460906982\n",
            "1.3105134963989258\n",
            "1.4328112602233887\n",
            "1.4393455982208252\n",
            "1.3884261846542358\n",
            "1.2270903587341309\n",
            "1.318135142326355\n",
            "1.3555779457092285\n",
            "1.4586704969406128\n",
            "1.466151475906372\n",
            "1.413893222808838\n",
            "1.4300363063812256\n",
            "1.3888702392578125\n",
            "1.3831050395965576\n",
            "1.2819104194641113\n",
            "1.3870303630828857\n",
            "1.383310079574585\n",
            "1.3614888191223145\n",
            "1.3961482048034668\n",
            "1.3563284873962402\n",
            "1.3856713771820068\n",
            "1.3261685371398926\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-7dbdf2ce1314>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mloss_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mMEG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWAV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSub\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTraining_Data_Batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mSub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBrainModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMEG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-7bdefa525040>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0midx_sound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_file\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mMEG_Signal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'MEG_Signals/S'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubjects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'T%03d.npy'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0midx_sound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'MEG_Signals/audios.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0maudios\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0m_ZIP_SUFFIX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb'PK\\x05\\x06'\u001b[0m \u001b[0;31m# empty zip files start with this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mmagic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;31m# If the file size is less than N, we need to make sure not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# to seek past the beginning of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}