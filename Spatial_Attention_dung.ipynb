{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sccn/sound2meg/blob/main/Spatial_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TRx3Md6j_YOX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "from scipy.io import loadmat\n",
    "from dataset_loading import Sound2MEGDataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import gc\n",
    "import sys\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "sys.tracebacklimit = 0\n",
    "\n",
    "class SubjectLayer(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SubjectLayer, self).__init__()\n",
    "    self.layers = []\n",
    "\n",
    "    for i in range(124): #124 subjects\n",
    "      layer = nn.Conv2d(270, 270, 1)\n",
    "      self.layers.append(layer)\n",
    "      \n",
    "  def forward(self, x, s_idx):\n",
    "    x = x.unsqueeze(1)\n",
    "    for i in range(len(x)):\n",
    "      x[i] = self.layers[s_idx[i]](x[i].clone())\n",
    "    return x[:, 0, :, :]\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "  def __init__(self,in_channels, out_channels, K, path):\n",
    "    super(SpatialAttention, self).__init__()\n",
    "    self.out = out_channels\n",
    "    self.input = in_channels\n",
    "    self.K = K\n",
    "    self.z = Parameter(torch.randn(self.out, K*K, dtype = torch.cfloat)/(32*32))\n",
    "    self.z.requires_grad = True\n",
    "    self.positions = loadmat(path + 'electrode_positions.mat')\n",
    "    self.positions = self.positions['positions']\n",
    "    self.x = torch.tensor(self.positions[:, 0]).to(device)\n",
    "    self.y = torch.tensor(self.positions[:, 1]).to(device)\n",
    "    self.cos_v = []\n",
    "    self.sin_v = []\n",
    "    self.cos = []\n",
    "    self.sin = []\n",
    "    for i in range(in_channels):\n",
    "      self.cos_v = []\n",
    "      self.sin_v = []\n",
    "      for k in range(K):\n",
    "        for l in range(K):\n",
    "          self.cos_v.append(torch.cos(2*math.pi*(k*self.x[i]+l*self.y[i])))\n",
    "          self.sin_v.append(torch.sin(2*math.pi*(k*self.x[i]+l*self.y[i])))\n",
    "      self.cos.append(torch.stack(self.cos_v))\n",
    "      self.sin.append(torch.stack(self.sin_v))\n",
    "    self.cos = torch.stack(self.cos).to(device)\n",
    "    self.sin = torch.stack(self.sin).to(device)\n",
    "  def forward(self, X):\n",
    "    N = X.size()[0]\n",
    "    SA = torch.zeros(N, 270, 360)\n",
    "    z_r = self.z.real\n",
    "    z_i = self.z.imag\n",
    "    a = (torch.mm(z_r.float(), torch.transpose(self.cos, 0, 1).float()) + torch.mm(z_i.float(), torch.transpose(self.sin, 0, 1).float())).to(device)\n",
    "    exp2 = torch.sum(torch.exp(a[:, 0:self.out]), 1).to(device)\n",
    "    exp2 = torch.transpose(exp2.unsqueeze(0), 0, 1)\n",
    "    exp2 = torch.mm(exp2, torch.ones(1, 360).to(device))\n",
    "    for i in range(N):\n",
    "      exp1 = torch.mm(torch.exp(a), X[i]).to(device)\n",
    "      SA[i] = exp1/exp2\n",
    "      #SA[i] = SpatialAttentionSoftmax(self.input, self.out, X[i], a)\n",
    "    return SA\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, path, F):\n",
    "    super(Net, self).__init__()\n",
    "    self.SA = SpatialAttention(273, 270, 32, path)\n",
    "    self.Subject = SubjectLayer()\n",
    "    self.F = F\n",
    "  def forward(self, y, s_idx):\n",
    "    x1 = self.SA(y).unsqueeze(0)\n",
    "    x2 = x1.permute((1, 2, 3, 0)) # subject attention?\n",
    "    x3 = nn.Conv2d(270, 270, (1, 1))(x2)\n",
    "    x = self.Subject(x3, s_idx)\n",
    "    for k in range(1,6):\n",
    "      p = pow(2,(2*k)%5)\n",
    "      q = pow(2,(2*k+1)%5)\n",
    "      if k == 1:\n",
    "        x = nn.Conv2d(270, 320, (3, 1), dilation = 1, padding = (1, 0))(x)\n",
    "        x = nn.BatchNorm2d(320)(x)\n",
    "        x = nn.GELU()(x)\n",
    "        x = nn.Conv2d(320, 320, (3, 1), dilation = 1, padding = (1, 0))(x)\n",
    "        x = nn.BatchNorm2d(320)(x)\n",
    "        x = nn.GELU()(x)\n",
    "        x = nn.Conv2d(320, 640, (3, 1), dilation = 2, padding = (2, 0))(x)\n",
    "        x = torch.transpose(x, 3, 1)\n",
    "        x = nn.GLU()(x)\n",
    "        x = torch.transpose(x, 3, 1)\n",
    "      else:\n",
    "        x1 = nn.Conv2d(320, 320, (3, 1), dilation = p, padding = (p, 0))(x)\n",
    "        x1 = nn.BatchNorm2d(320)(x1)\n",
    "        x1 = nn.GELU()(x1)\n",
    "        x2 = x + x1\n",
    "        x3 = nn.Conv2d(320, 320, (3, 1), dilation = q, padding = (q, 0))(x2)\n",
    "        x3 = nn.BatchNorm2d(320)(x3)\n",
    "        x3 = nn.GELU()(x3)\n",
    "        x4 = x2 + x2\n",
    "        x_out = nn.Conv2d(320, 640, (3, 1), dilation = 2, padding = (2, 0))(x4)\n",
    "        x_out = torch.transpose(x_out, 3, 1)\n",
    "        x_out = nn.GLU()(x_out)\n",
    "        x_out = torch.transpose(x_out, 3, 1)\n",
    "    x_out = nn.Conv2d(320, 640, (1, 1))(x_out)\n",
    "    x_out = nn.GELU()(x_out)\n",
    "    x_out = nn.Conv2d(640, self.F, (1, 1))(x_out)\n",
    "    return x_out\n",
    "\n",
    "def CLIP_loss(Z, Y):\n",
    "  N = Y.size(dim = 0)\n",
    "  #inner_product = torch.zeros(N, N)\n",
    "  log_softmax = torch.zeros(N).to(device)\n",
    "  Z_row = torch.reshape(Z, (N, -1)).to(device)\n",
    "  Y_row = torch.reshape(Y, (N, -1)).to(device)\n",
    "  inner_product = (torch.mm(Z_row, torch.transpose(Y_row, 1, 0))/(N*N)).to(device)\n",
    "  for i in range(N):\n",
    "    inn = inner_product[i, :].to(device)\n",
    "    log_softmax[i] = torch.log(nn.functional.softmax(inn, -1))[i]\n",
    "  return sum(-1*log_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 744
    },
    "id": "KJ5iCaaqHjAK",
    "outputId": "c5b8a99b-e817-4a50-ca19-9e99be22d865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621.0598754882812\n",
      "621.0598754882812\n",
      "621.0599365234375\n",
      "621.0599975585938\n",
      "621.0598754882812\n",
      "621.0598754882812\n",
      "621.0598754882812\n",
      "621.0598754882812\n",
      "621.0598754882812\n",
      "621.0599975585938\n",
      "621.0599365234375\n",
      "621.0599365234375\n",
      "621.0599365234375\n",
      "621.0599365234375\n",
      "621.0598754882812\n"
     ]
    }
   ],
   "source": [
    "dataset = Sound2MEGDataset('/expanse/projects/nsg/external_users/public/arno/')\n",
    "training_data, validation_data, test_data = random_split(dataset, [11497, 3285, 1642], generator=torch.Generator().manual_seed(42))\n",
    "Training_Data_Batches = DataLoader(training_data, batch_size = 128, shuffle = True)\n",
    "Validation_Data_Batches = DataLoader(validation_data, batch_size = 128, shuffle = True)\n",
    "BrainModule = Net('/expanse/projects/nsg/external_users/public/arno/', 120)\n",
    "BrainModule.to(device)\n",
    "optimizer = optim.Adam(BrainModule.parameters(), lr = 0.0003)\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "\n",
    "optimizer.zero_grad()\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for i in range(10):\n",
    "    loss_t = 0\n",
    "    loss_v = 0\n",
    "    for j in range(13):\n",
    "        for MEG, WAV, Sub in Training_Data_Batches:\n",
    "            Sub = Sub.tolist()\n",
    "            Z = BrainModule(MEG.to(device), Sub)\n",
    "            Z = Z[:, :, :, 0]\n",
    "            loss = CLIP_loss(Z.float(), WAV.abs().float().to(device))\n",
    "            print(loss.item())\n",
    "            loss.backward()\n",
    "            loss_t = loss_t + loss.item()\n",
    "            optimizer.step()\n",
    "    loss_train.append(loss_t/(13*len(Training_Data_Batches)))\n",
    "    for MEG_val, WAV_val, Sub_val in Validation_Data_Batches:\n",
    "        with torch.no_grad():\n",
    "            Z_val = BrainModule(MEG_val.to(device), Sub_val)\n",
    "            loss = CLIP_loss(Z_val.float(), WAV_val.abs().float().to(device))\n",
    "    print(loss.item())\n",
    "    loss_v = loss_v + loss.item()\n",
    "    loss_val.append(loss_v/len(Validation_Data_Batches))\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(loss_train)\n",
    "print(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "sizes = np.empty(126)\n",
    "for subject in range(0, 126):\n",
    "    if os.path.exists(filePath + 'MEG_Signals/S%03dT000.npy'%subject):\n",
    "        files = glob.glob(filePath + 'MEG_Signals/S%03dT*.npy'%subject)\n",
    "        sizes[subject] = len(files)\n",
    "    else:\n",
    "        sizes[subject] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dl]",
   "language": "python",
   "name": "conda-env-.conda-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
