{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sccn/sound2meg/blob/main/Spatial_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bf159080e069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cb7a5402dcf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.arange(10)\n",
    "torch.randint(5, (3,), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRx3Md6j_YOX",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621.0598754882812\n",
      "621.0598754882812\n",
      "621.0599365234375\n",
      "621.059814453125\n",
      "621.059814453125\n",
      "621.0599365234375\n",
      "621.06005859375\n",
      "621.06005859375\n",
      "621.0601196289062\n",
      "621.0599365234375\n",
      "621.059814453125\n",
      "621.0599365234375\n",
      "621.0599365234375\n",
      "621.059814453125\n",
      "621.0599975585938\n",
      "621.0598754882812\n",
      "621.06005859375\n",
      "621.0596923828125\n",
      "621.06005859375\n",
      "621.06005859375\n",
      "621.059814453125\n",
      "621.0599365234375\n",
      "621.059814453125\n",
      "621.0601196289062\n",
      "621.0599365234375\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Spatial_Attention.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/github/sccn/sound2meg/blob/main/Spatial_Attention.ipynb\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "from scipy.io import loadmat\n",
    "from dataset_loading import Sound2MEGDataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import gc\n",
    "import sys\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "sys.tracebacklimit = 0\n",
    "\n",
    "class SubjectLayer(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SubjectLayer, self).__init__()\n",
    "    self.layers = []\n",
    "\n",
    "    for i in range(124): #124 subjects\n",
    "      layer = nn.Conv2d(270, 270, 1)\n",
    "      layer.to(device)\n",
    "      self.layers.append(layer)\n",
    "      \n",
    "  def forward(self, x, s_idx):\n",
    "    x = x.unsqueeze(1)\n",
    "    for i in range(len(x)):\n",
    "      x[i] = self.layers[s_idx[i]](x[i].clone())\n",
    "    return x[:, 0, :, :]\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "  def __init__(self,in_channels, out_channels, K, path):\n",
    "    super(SpatialAttention, self).__init__()\n",
    "    self.out = out_channels\n",
    "    self.input = in_channels\n",
    "    self.K = K\n",
    "    self.z = Parameter(torch.randn(self.out, K*K, dtype = torch.cfloat, device=device)/(32*32))\n",
    "    self.z.requires_grad = True\n",
    "    self.positions = loadmat(path + 'electrode_positions.mat')\n",
    "    self.positions = self.positions['positions']\n",
    "    self.x = torch.tensor(self.positions[:, 0]).to(device)\n",
    "    self.y = torch.tensor(self.positions[:, 1]).to(device)\n",
    "    self.cos_v = []\n",
    "    self.sin_v = []\n",
    "    self.cos = []\n",
    "    self.sin = []\n",
    "    for i in range(in_channels):\n",
    "      self.cos_v = []\n",
    "      self.sin_v = []\n",
    "      for k in range(K):\n",
    "        for l in range(K):\n",
    "          self.cos_v.append(torch.cos(2*math.pi*(k*self.x[i]+l*self.y[i])))\n",
    "          self.sin_v.append(torch.sin(2*math.pi*(k*self.x[i]+l*self.y[i])))\n",
    "      self.cos.append(torch.stack(self.cos_v))\n",
    "      self.sin.append(torch.stack(self.sin_v))\n",
    "    self.cos = torch.stack(self.cos).to(device)\n",
    "    self.sin = torch.stack(self.sin).to(device)\n",
    "  def forward(self, X):\n",
    "    N = X.size()[0]\n",
    "    SA = torch.zeros(N, 270, 360, device=device)\n",
    "    z_r = self.z.real\n",
    "    z_i = self.z.imag\n",
    "    a = (torch.mm(z_r.float(), torch.transpose(self.cos, 0, 1).float()) + torch.mm(z_i.float(), torch.transpose(self.sin, 0, 1).float())).to(device)\n",
    "    exp2 = torch.sum(torch.exp(a[:, 0:self.out]), 1).to(device)\n",
    "    exp2 = torch.transpose(exp2.unsqueeze(0), 0, 1)\n",
    "    exp2 = torch.mm(exp2, torch.ones(1, 360).to(device))\n",
    "    for i in range(N):\n",
    "      exp1 = torch.mm(torch.exp(a), X[i]).to(device)\n",
    "      SA[i] = exp1/exp2\n",
    "      #SA[i] = SpatialAttentionSoftmax(self.input, self.out, X[i], a)\n",
    "    return SA\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, path, F):\n",
    "    super(Net, self).__init__()\n",
    "    self.SA = SpatialAttention(273, 270, 32, path)\n",
    "    self.SA.to(device)\n",
    "    self.Subject = SubjectLayer()\n",
    "    self.Subject.to(device)\n",
    "    self.F = F\n",
    "    self.conv1 = nn.Conv2d(270, 270, (1, 1)).to(device)\n",
    "    self.conv2 = nn.Conv2d(320, 640, (1, 1)).to(device)\n",
    "    self.conv3 = nn.Conv2d(640, self.F, (1, 1)).to(device)\n",
    "    self.gelu = nn.GELU().to(device)\n",
    "    self.loop_convs = []\n",
    "    self.loop_batchnorms = []\n",
    "    self.loop_gelus = []\n",
    "    self.loop_glus = []\n",
    "    for k in range(1, 6):\n",
    "      p = pow(2,(2*k)%5)\n",
    "      q = pow(2,(2*k+1)%5)\n",
    "      self.convs = []\n",
    "      self.batchnorms = []\n",
    "      self.gelus = []\n",
    "      self.convs.append(nn.Conv2d(320, 320, (3, 1), dilation = p, padding = (p, 0)).to(device))\n",
    "      self.convs.append(nn.Conv2d(320, 320, (3, 1), dilation = q, padding = (q, 0)).to(device))\n",
    "      self.convs.append(nn.Conv2d(320, 640, (3, 1), dilation = 2, padding = (2, 0)).to(device))\n",
    "      for i in range(2):\n",
    "        self.batchnorms.append(nn.BatchNorm2d(320).to(device))\n",
    "        self.gelus.append(nn.GELU().to(device))\n",
    "      self.loop_convs.append(self.convs)\n",
    "      self.loop_batchnorms.append(self.batchnorms)\n",
    "      self.loop_gelus.append(self.gelus)\n",
    "      self.loop_glus.append(nn.GLU().to(device))\n",
    "    self.loop_convs[0][0] = nn.Conv2d(270, 320, (3, 1), dilation = 1, padding = (1, 0)).to(device)\n",
    "  def forward(self, x, s_idx):\n",
    "    x = self.SA(x).unsqueeze(3)\n",
    "    x = self.conv1(x)\n",
    "    x = self.Subject(x, s_idx)\n",
    "    for i in range(5):\n",
    "      if i == 0:\n",
    "        x = self.loop_convs[0][0](x)\n",
    "        x = self.loop_batchnorms[0][0](x)\n",
    "        x = self.loop_gelus[0][0](x)\n",
    "        x = self.loop_convs[0][1](x)\n",
    "        x = self.loop_batchnorms[0][1](x)\n",
    "        x = self.loop_gelus[0][1](x)\n",
    "        x = self.loop_convs[0][2](x)\n",
    "        x = torch.transpose(x, 3, 1)\n",
    "        x = self.loop_glus[0](x)\n",
    "        x = torch.transpose(x, 3, 1)\n",
    "      else:\n",
    "        x1 = self.loop_convs[i][0](x)\n",
    "        x1 = self.loop_batchnorms[i][0](x)\n",
    "        x1 = self.loop_gelus[i][0](x)\n",
    "        x2 = x + x1\n",
    "        x3 = self.loop_convs[i][1](x2)\n",
    "        x3 = self.loop_batchnorms[i][1](x3)\n",
    "        x3 = self.loop_gelus[i][1](x3)\n",
    "        x4 = x2 + x3\n",
    "        x5 = self.loop_convs[i][2](x4)\n",
    "        x5 = torch.transpose(x5, 3, 1)\n",
    "        x5 = self.loop_glus[i](x5)\n",
    "        x = torch.transpose(x5, 3, 1)\n",
    "    x_out = self.conv2(x)\n",
    "    x_out = self.gelu(x_out)\n",
    "    x_out = self.conv3(x_out)\n",
    "    return x_out\n",
    "\n",
    "def CLIP_loss(Z, Y):\n",
    "  N = Y.size(dim = 0)\n",
    "  #inner_product = torch.zeros(N, N)\n",
    "  log_softmax = torch.zeros(N).to(device)\n",
    "  Z_row = torch.reshape(Z, (N, -1)).to(device)\n",
    "  Y_row = torch.reshape(Y, (N, -1)).to(device)\n",
    "  inner_product = (torch.mm(Z_row, torch.transpose(Y_row, 1, 0))/(N*N)).to(device)\n",
    "  for i in range(N):\n",
    "    inn = inner_product[i, :].to(device)\n",
    "    log_softmax[i] = torch.log(nn.functional.softmax(inn, -1))[i]\n",
    "  return sum(-1*log_softmax)\n",
    "\n",
    "Dataset = Sound2MEGDataset('/expanse/projects/nsg/external_users/public/arno/')\n",
    "training_data, validation_data, test_data = random_split(Dataset, [11497, 3285, 1642], generator=torch.Generator().manual_seed(42))\n",
    "Training_Data_Batches = DataLoader(training_data, batch_size = 128, shuffle = True)\n",
    "Validation_Data_Batches = DataLoader(validation_data, batch_size = 128, shuffle = True)\n",
    "BrainModule = Net('/expanse/projects/nsg/external_users/public/arno/', 120)\n",
    "BrainModule.to(device)\n",
    "optimizer = optim.Adam(BrainModule.parameters(), lr = 0.0003)\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "\n",
    "for i in range(10):\n",
    "  loss_t = 0\n",
    "  loss_v = 0\n",
    "  for j in range(13):\n",
    "    for MEG, WAV, Sub in Training_Data_Batches:\n",
    "      Sub = Sub.tolist()\n",
    "      Z = BrainModule(MEG.to(device), Sub)\n",
    "      Z = Z[:, :, :, 0]\n",
    "      loss = CLIP_loss(Z.float(), WAV.abs().float().to(device))\n",
    "      print(loss.item())\n",
    "      torch.autograd.set_detect_anomaly(True)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      loss_t = loss_t + loss.item()\n",
    "      optimizer.step()\n",
    "  loss_train.append(loss_t/(13*len(Training_Data_Batches)))\n",
    "  for MEG_val, WAV_val, Sub_val in Validation_Data_Batches:\n",
    "    Z_val = BrainModule(MEG_val.to(device), Sub_val)\n",
    "    loss = CLIP_loss(Z_val.float(), WAV_val.abs().float().to(device))\n",
    "    print(loss.item())\n",
    "    loss_v = loss_v + loss.item()\n",
    "  loss_val.append(loss_v/len(Validation_Data_Batches))\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "print(loss_train)\n",
    "print(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
