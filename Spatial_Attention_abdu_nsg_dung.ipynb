{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sccn/sound2meg/blob/main/Spatial_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TRx3Md6j_YOX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Spatial_Attention.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/github/sccn/sound2meg/blob/main/Spatial_Attention.ipynb\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "from scipy.io import loadmat\n",
    "from dataset_loading import Sound2MEGDataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import gc\n",
    "import sys\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "sys.tracebacklimit = 0\n",
    "\n",
    "class SubjectLayer(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SubjectLayer, self).__init__()\n",
    "    self.layers = []\n",
    "\n",
    "    for i in range(124): #124 subjects\n",
    "      layer = nn.Conv2d(270, 270, 1).to(device)\n",
    "      self.layers.append(layer)\n",
    "      \n",
    "  def forward(self, x, s_idx):\n",
    "    x = x.unsqueeze(1)\n",
    "    for i in range(len(x)):\n",
    "      x[i] = self.layers[s_idx[i]](x[i].clone())\n",
    "    return x[:, 0, :, :]\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "  def __init__(self,in_channels, out_channels, K, path):\n",
    "    super(SpatialAttention, self).__init__()\n",
    "    self.out = out_channels\n",
    "    self.input = in_channels\n",
    "    self.K = K\n",
    "    self.z = Parameter(torch.randn(self.out, K*K, dtype = torch.cfloat)/(32*32))\n",
    "    self.z.requires_grad = True\n",
    "    self.positions = loadmat(path + 'electrode_positions.mat')\n",
    "    self.positions = self.positions['positions']\n",
    "    self.x = torch.tensor(self.positions[:, 0]).to(device)\n",
    "    self.y = torch.tensor(self.positions[:, 1]).to(device)\n",
    "    self.cos_v = []\n",
    "    self.sin_v = []\n",
    "    self.cos = []\n",
    "    self.sin = []\n",
    "    for i in range(in_channels):\n",
    "      self.cos_v = []\n",
    "      self.sin_v = []\n",
    "      for k in range(K):\n",
    "        for l in range(K):\n",
    "          self.cos_v.append(torch.cos(2*math.pi*(k*self.x[i]+l*self.y[i])))\n",
    "          self.sin_v.append(torch.sin(2*math.pi*(k*self.x[i]+l*self.y[i])))\n",
    "      self.cos.append(torch.stack(self.cos_v))\n",
    "      self.sin.append(torch.stack(self.sin_v))\n",
    "    self.cos = torch.stack(self.cos).to(device)\n",
    "    self.sin = torch.stack(self.sin).to(device)\n",
    "  def forward(self, X):\n",
    "    N = X.size()[0]\n",
    "    SA = torch.zeros(N, 270, 360).to(device)\n",
    "    z_r = self.z.real\n",
    "    z_i = self.z.imag\n",
    "    a = (torch.mm(z_r.float(), torch.transpose(self.cos, 0, 1).float()) + torch.mm(z_i.float(), torch.transpose(self.sin, 0, 1).float())).to(device)\n",
    "    exp2 = torch.sum(torch.exp(a[:, 0:self.out]), 1).to(device)\n",
    "    exp2 = torch.transpose(exp2.unsqueeze(0), 0, 1)\n",
    "    exp2 = torch.mm(exp2, torch.ones(1, 360).to(device))\n",
    "    for i in range(N):\n",
    "      exp1 = torch.mm(torch.exp(a), X[i]).to(device)\n",
    "      SA[i] = (exp1/exp2).to(device)\n",
    "      #SA[i] = SpatialAttentionSoftmax(self.input, self.out, X[i], a)\n",
    "    return SA\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, path, F):\n",
    "    super(Net, self).__init__()\n",
    "    self.SA = SpatialAttention(273, 270, 32, path)\n",
    "    self.Subject = SubjectLayer().to(device)\n",
    "    self.F = F\n",
    "    self.conv1 = nn.Conv2d(270, 270, (1, 1)).to(device)\n",
    "    self.conv2 = nn.Conv2d(320, 640, (1, 1)).to(device)\n",
    "    self.conv3 = nn.Conv2d(640, self.F, (1, 1)).to(device)\n",
    "    self.gelu = nn.GELU().to(device)\n",
    "    self.loop_convs = []\n",
    "    self.loop_batchnorms = []\n",
    "    self.loop_gelus = []\n",
    "    self.loop_glus = []\n",
    "    for k in range(1, 6):\n",
    "      p = pow(2,(2*k)%5)\n",
    "      q = pow(2,(2*k+1)%5)\n",
    "      self.convs = []\n",
    "      self.batchnorms = []\n",
    "      self.gelus = []\n",
    "      self.convs.append(nn.Conv2d(320, 320, (3, 1), dilation = p, padding = (p, 0)).to(device))\n",
    "      self.convs.append(nn.Conv2d(320, 320, (3, 1), dilation = q, padding = (q, 0)).to(device))\n",
    "      self.convs.append(nn.Conv2d(320, 640, (3, 1), dilation = 2, padding = (2, 0)).to(device))\n",
    "      for i in range(2):\n",
    "        self.batchnorms.append(nn.BatchNorm2d(320).to(device))\n",
    "        self.gelus.append(nn.GELU().to(device))\n",
    "      self.loop_convs.append(self.convs)\n",
    "      self.loop_batchnorms.append(self.batchnorms)\n",
    "      self.loop_gelus.append(self.gelus)\n",
    "      self.loop_glus.append(nn.GLU().to(device))\n",
    "    self.loop_convs[0][0] = nn.Conv2d(270, 320, (3, 1), dilation = 1, padding = (1, 0)).to(device)\n",
    "  def forward(self, x, s_idx):\n",
    "    x = self.SA(x).unsqueeze(3)\n",
    "    x = self.conv1(x)\n",
    "    x = self.Subject(x, s_idx)\n",
    "    for i in range(5):\n",
    "      if i == 0:\n",
    "        x = self.loop_convs[0][0](x)\n",
    "        x = self.loop_batchnorms[0][0](x)\n",
    "        x = self.loop_gelus[0][0](x)\n",
    "        x = self.loop_convs[0][1](x)\n",
    "        x = self.loop_batchnorms[0][1](x)\n",
    "        x = self.loop_gelus[0][1](x)\n",
    "        x = self.loop_convs[0][2](x)\n",
    "        x = torch.transpose(x, 3, 1)\n",
    "        x = self.loop_glus[0](x)\n",
    "        x = torch.transpose(x, 3, 1)\n",
    "      else:\n",
    "        x1 = self.loop_convs[i][0](x)\n",
    "        x1 = self.loop_batchnorms[i][0](x)\n",
    "        x1 = self.loop_gelus[i][0](x)\n",
    "        x2 = x + x1\n",
    "        x3 = self.loop_convs[i][1](x2)\n",
    "        x3 = self.loop_batchnorms[i][1](x3)\n",
    "        x3 = self.loop_gelus[i][1](x3)\n",
    "        x4 = x2 + x3\n",
    "        x5 = self.loop_convs[i][2](x4)\n",
    "        x5 = torch.transpose(x5, 3, 1)\n",
    "        x5 = self.loop_glus[i](x5)\n",
    "        x = torch.transpose(x5, 3, 1)\n",
    "    x_out = self.conv2(x)\n",
    "    x_out = self.gelu(x_out)\n",
    "    x_out = self.conv3(x_out)\n",
    "    return x_out\n",
    "\n",
    "# def CLIP_loss(Z, Y):\n",
    "#   N = Y.size(dim = 0)\n",
    "#   #inner_product = torch.zeros(N, N)\n",
    "#   log_softmax = torch.zeros(N).to(device)\n",
    "#   Z_row = torch.reshape(Z, (N, -1)).to(device)\n",
    "#   Y_row = torch.reshape(Y, (N, -1)).to(device)\n",
    "#   inner_product = (torch.mm(Z_row, torch.transpose(Y_row, 1, 0))).to(device)\n",
    "#   for i in range(N):\n",
    "#     inn = inner_product[i, :].to(device)\n",
    "#     log_softmax[i] = torch.log(nn.functional.softmax(inn, -1).clamp(min=1e-4))[i]\n",
    "#   return sum(-1*log_softmax)\n",
    "\n",
    "def CLIP_loss(Z, Y, device):\n",
    "    '''\n",
    "    New loss using cross entropy implementation\n",
    "    '''\n",
    "    N = Y.size(dim = 0) # batch size\n",
    "    log_softmax = torch.zeros(N, device=device)\n",
    "    Z_row = torch.reshape(Z, (N, -1)) # flatten to be N x F\n",
    "    Y_row = torch.reshape(Y, (N, -1)) # flatten to be N x F\n",
    "    inner_product = torch.mm(Z_row, Y_row.T)/(N*N) # N x N. The normalization?\n",
    "\n",
    "    target = torch.arange(N, device=device)\n",
    "    loss_brain = torch.nn.functional.cross_entropy(inner_product, target)\n",
    "    loss_sound = torch.nn.functional.cross_entropy(inner_product.T, target)\n",
    "    loss = (loss_brain + loss_sound)/2\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: [4.849829763836331]\n",
      "Val loss: [4.836285150968111]\n",
      "Train loss: [4.849829763836331, 4.849829408857557]\n",
      "Val loss: [4.836285150968111, 4.836284765830407]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576, 4.849234469731649]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576, 4.849234469731649, 4.849193599489]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498, 4.836349597344031]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498, 4.836349597344031, 4.836390256881714]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576, 4.849234469731649, 4.849193599489, 4.84919802347819, 4.849103736877441, 4.849090327156914]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498, 4.836349597344031, 4.836390256881714, 4.836256155600915]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576, 4.849234469731649, 4.849193599489, 4.84919802347819, 4.849103736877441, 4.849090327156914, 4.849005450142754]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498, 4.836349597344031, 4.836390256881714, 4.836256155600915, 4.836112902714656]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576, 4.849234469731649, 4.849193599489, 4.84919802347819, 4.849103736877441, 4.849090327156914, 4.849005450142754, 4.848889912499322]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498, 4.836349597344031, 4.836390256881714, 4.836256155600915, 4.836112902714656, 4.836172562379104]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576, 4.849234469731649, 4.849193599489, 4.84919802347819, 4.849103736877441, 4.849090327156914, 4.849005450142754, 4.848889912499322, 4.848411189185248]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498, 4.836349597344031, 4.836390256881714, 4.836256155600915, 4.836112902714656, 4.836172562379104, 4.835977994478666]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576, 4.849234469731649, 4.849193599489, 4.84919802347819, 4.849103736877441, 4.849090327156914, 4.849005450142754, 4.848889912499322, 4.848411189185248, 4.846921814812554]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498, 4.836349597344031, 4.836390256881714, 4.836256155600915, 4.836112902714656, 4.836172562379104, 4.835977994478666, 4.836156166516817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576, 4.849234469731649, 4.849193599489, 4.84919802347819, 4.849103736877441, 4.849090327156914, 4.849005450142754, 4.848889912499322, 4.848411189185248, 4.846921814812554, 4.8452170001135935]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498, 4.836349597344031, 4.836390256881714, 4.836256155600915, 4.836112902714656, 4.836172562379104, 4.835977994478666, 4.836156166516817, 4.836175845219539]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576, 4.849234469731649, 4.849193599489, 4.84919802347819, 4.849103736877441, 4.849090327156914, 4.849005450142754, 4.848889912499322, 4.848411189185248, 4.846921814812554, 4.8452170001135935, 4.843177678849962]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498, 4.836349597344031, 4.836390256881714, 4.836256155600915, 4.836112902714656, 4.836172562379104, 4.835977994478666, 4.836156166516817, 4.836175845219539, 4.836454978355994]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576, 4.849234469731649, 4.849193599489, 4.84919802347819, 4.849103736877441, 4.849090327156914, 4.849005450142754, 4.848889912499322, 4.848411189185248, 4.846921814812554, 4.8452170001135935, 4.843177678849962, 4.84049326048957]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498, 4.836349597344031, 4.836390256881714, 4.836256155600915, 4.836112902714656, 4.836172562379104, 4.835977994478666, 4.836156166516817, 4.836175845219539, 4.836454978355994, 4.836473685044509]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576, 4.849234469731649, 4.849193599489, 4.84919802347819, 4.849103736877441, 4.849090327156914, 4.849005450142754, 4.848889912499322, 4.848411189185248, 4.846921814812554, 4.8452170001135935, 4.843177678849962, 4.84049326048957, 4.837433830897013]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498, 4.836349597344031, 4.836390256881714, 4.836256155600915, 4.836112902714656, 4.836172562379104, 4.835977994478666, 4.836156166516817, 4.836175845219539, 4.836454978355994, 4.836473685044509, 4.838052382835975]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576, 4.849234469731649, 4.849193599489, 4.84919802347819, 4.849103736877441, 4.849090327156914, 4.849005450142754, 4.848889912499322, 4.848411189185248, 4.846921814812554, 4.8452170001135935, 4.843177678849962, 4.84049326048957, 4.837433830897013, 4.834198109308878]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498, 4.836349597344031, 4.836390256881714, 4.836256155600915, 4.836112902714656, 4.836172562379104, 4.835977994478666, 4.836156166516817, 4.836175845219539, 4.836454978355994, 4.836473685044509, 4.838052382835975, 4.838922867408166]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576, 4.849234469731649, 4.849193599489, 4.84919802347819, 4.849103736877441, 4.849090327156914, 4.849005450142754, 4.848889912499322, 4.848411189185248, 4.846921814812554, 4.8452170001135935, 4.843177678849962, 4.84049326048957, 4.837433830897013, 4.834198109308878, 4.830037413703071]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498, 4.836349597344031, 4.836390256881714, 4.836256155600915, 4.836112902714656, 4.836172562379104, 4.835977994478666, 4.836156166516817, 4.836175845219539, 4.836454978355994, 4.836473685044509, 4.838052382835975, 4.838922867408166, 4.840368270874023]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576, 4.849234469731649, 4.849193599489, 4.84919802347819, 4.849103736877441, 4.849090327156914, 4.849005450142754, 4.848889912499322, 4.848411189185248, 4.846921814812554, 4.8452170001135935, 4.843177678849962, 4.84049326048957, 4.837433830897013, 4.834198109308878, 4.830037413703071, 4.825919739405314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498, 4.836349597344031, 4.836390256881714, 4.836256155600915, 4.836112902714656, 4.836172562379104, 4.835977994478666, 4.836156166516817, 4.836175845219539, 4.836454978355994, 4.836473685044509, 4.838052382835975, 4.838922867408166, 4.840368270874023, 4.843229733980619]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576, 4.849234469731649, 4.849193599489, 4.84919802347819, 4.849103736877441, 4.849090327156914, 4.849005450142754, 4.848889912499322, 4.848411189185248, 4.846921814812554, 4.8452170001135935, 4.843177678849962, 4.84049326048957, 4.837433830897013, 4.834198109308878, 4.830037413703071, 4.825919739405314, 4.821264213985867]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498, 4.836349597344031, 4.836390256881714, 4.836256155600915, 4.836112902714656, 4.836172562379104, 4.835977994478666, 4.836156166516817, 4.836175845219539, 4.836454978355994, 4.836473685044509, 4.838052382835975, 4.838922867408166, 4.840368270874023, 4.843229733980619, 4.844569169558012]\n",
      "Train loss: [4.849829763836331, 4.849829408857557, 4.849828423394097, 4.849826018015544, 4.849814383188884, 4.849751334720188, 4.84974873330858, 4.849783256318834, 4.8497604264153376, 4.849728107452393, 4.849700111813015, 4.849664497375488, 4.849654097027249, 4.849597915013631, 4.849529012044271, 4.849509366353353, 4.8494378884633385, 4.8494542492760555, 4.849411741892497, 4.849388064278497, 4.849312114715576, 4.849234469731649, 4.849193599489, 4.84919802347819, 4.849103736877441, 4.849090327156914, 4.849005450142754, 4.848889912499322, 4.848411189185248, 4.846921814812554, 4.8452170001135935, 4.843177678849962, 4.84049326048957, 4.837433830897013, 4.834198109308878, 4.830037413703071, 4.825919739405314, 4.821264213985867, 4.816233274671767]\n",
      "Val loss: [4.836285150968111, 4.836284765830407, 4.836284490732046, 4.836283206939697, 4.83628170306866, 4.836259016623864, 4.836259255042443, 4.836244289691631, 4.836200108894935, 4.836186849153959, 4.836246142020593, 4.836316090363723, 4.8362516439878025, 4.836211626346294, 4.836219659218421, 4.8363142380347615, 4.836036315331092, 4.836402764687171, 4.836247444152832, 4.836460113525391, 4.836268003170307, 4.836495142716628, 4.836570336268498, 4.836349597344031, 4.836390256881714, 4.836256155600915, 4.836112902714656, 4.836172562379104, 4.835977994478666, 4.836156166516817, 4.836175845219539, 4.836454978355994, 4.836473685044509, 4.838052382835975, 4.838922867408166, 4.840368270874023, 4.843229733980619, 4.844569169558012, 4.845232890202449]\n"
     ]
    }
   ],
   "source": [
    "dataset = Sound2MEGDataset('/expanse/projects/nsg/external_users/public/arno/')\n",
    "training_data, validation_data, test_data = random_split(dataset, [11497, 3285, 1642], generator=torch.Generator().manual_seed(42))\n",
    "Training_Data_Batches = DataLoader(training_data, batch_size = 128, shuffle = True)\n",
    "Validation_Data_Batches = DataLoader(validation_data, batch_size = 128, shuffle = True)\n",
    "BrainModule = Net('/expanse/projects/nsg/external_users/public/arno/', 120)\n",
    "BrainModule.to(device)\n",
    "optimizer = optim.Adam(BrainModule.parameters(), lr = 0.0003)\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "\n",
    "for i in range(50):\n",
    "  loss_t = 0\n",
    "  loss_v = 0\n",
    "  for MEG, WAV, Sub in Training_Data_Batches:\n",
    "    Sub = Sub.tolist()\n",
    "    optimizer.zero_grad()\n",
    "    Z = BrainModule(MEG.to(device), Sub)\n",
    "    Z = Z[:, :, :, 0]\n",
    "    loss = CLIP_loss(Z.float(), WAV.abs().float().to(device), device)\n",
    "    loss.backward()\n",
    "    loss_t = loss_t + loss.item()\n",
    "    optimizer.step()\n",
    "  loss_train.append(loss_t/(len(Training_Data_Batches)))\n",
    "  print(\"Train loss:\",loss_train)\n",
    "  for MEG_val, WAV_val, Sub_val in Validation_Data_Batches:\n",
    "    with torch.no_grad():\n",
    "      Z_val = BrainModule(MEG_val.to(device), Sub_val)\n",
    "      loss = CLIP_loss(Z_val.float(), WAV_val.abs().float().to(device), device)\n",
    "    loss_v = loss_v + loss.item()\n",
    "  loss_val.append(loss_v/len(Validation_Data_Batches))\n",
    "  print(\"Val loss:\",loss_val)\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "print(loss_train)\n",
    "print(loss_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dl]",
   "language": "python",
   "name": "conda-env-.conda-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
